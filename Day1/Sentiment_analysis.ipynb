{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aed9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hugging Face token for all API calls\n",
    "import os\n",
    "os.environ['HF_TOKEN'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0577f38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7202f907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9998059868812561}]\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "classifier = pipeline(\"sentiment-analysis\",model = \"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "text = \"It is a very bad user interface in Google colab\"\n",
    "result = classifier(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04657796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---generate0---\n",
      "{'generated_text': \"Who is Gandhi?\\n\\nGandhi's legacy will be on display when the next president of India takes office next year. He has a unique history as one of the world's most famous leaders. He took a country on a mission, and it was to liberate the world from the scourge of corruption and to save lives. He is a visionary.\\n\\nThe current president of India is also a great leader\"}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "generator = pipeline(\"text-generation\",model = \"gpt2\")\n",
    "prompt = input(\"Ask Anything : \")\n",
    "outputs = generator(prompt,max_new_tokens = 80,do_sample = True,\n",
    "                    temperature = 0.8 ,top_p = 0.9)\n",
    "for i,out in enumerate(outputs):\n",
    "    print(f\"---generate{i}---\")\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4195c4fd",
   "metadata": {},
   "source": [
    "# Question Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d04c4d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.009186969138681889, 'start': 279, 'end': 284, 'answer': 'India'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "cbot = pipeline(\"question-answering\",model = \"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "context = input(\"Enter Your Content here : \")\n",
    "prompt  = input(\"Ask question about you context: \")\n",
    "\n",
    "answer = cbot(question = prompt,context = context)\n",
    "print(answer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6387f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://huggingface.co/api/models/mistralai/Mistral-7B-Instruct-v0.3?expand=inferenceProviderMapping (Request ID: Root=1-68b1f472-34a312c41958c5f36a6619d6;f4481049-c0f3-4472-8610-f3190bc0cb14)\n\nInvalid credentials in Authorization header",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ravik\\OneDrive\\Desktop\\Agentic_Ai_Session\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ravik\\OneDrive\\Desktop\\Agentic_Ai_Session\\venv\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/mistralai/Mistral-7B-Instruct-v0.3?expand=inferenceProviderMapping",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     19\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(resp)\n\u001b[32m     21\u001b[39m message=[\n\u001b[32m     22\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mYou are a helpful Leetcode Problem Solver\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     23\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mGive me Add Two Numbers Problem solution\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m     24\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m resp=\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(extract_text(resp))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ravik\\OneDrive\\Desktop\\Agentic_Ai_Session\\venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:886\u001b[39m, in \u001b[36mInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    883\u001b[39m payload_model = model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Get the provider helper\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m886\u001b[39m provider_helper = \u001b[43mget_provider_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconversational\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id_or_url\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_id_or_url\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_id_or_url\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttp://\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpayload_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[38;5;66;03m# Prepare the payload\u001b[39;00m\n\u001b[32m    895\u001b[39m parameters = {\n\u001b[32m    896\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: payload_model,\n\u001b[32m    897\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   (...)\u001b[39m\u001b[32m    914\u001b[39m     **(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    915\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ravik\\OneDrive\\Desktop\\Agentic_Ai_Session\\venv\\Lib\\site-packages\\huggingface_hub\\inference\\_providers\\__init__.py:195\u001b[39m, in \u001b[36mget_provider_helper\u001b[39m\u001b[34m(provider, task, model)\u001b[39m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    194\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSpecifying a model is required when provider is \u001b[39m\u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     provider_mapping = \u001b[43m_fetch_inference_provider_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m     provider = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(provider_mapping)).provider\n\u001b[32m    198\u001b[39m provider_tasks = PROVIDERS.get(provider)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ravik\\OneDrive\\Desktop\\Agentic_Ai_Session\\venv\\Lib\\site-packages\\huggingface_hub\\inference\\_providers\\_common.py:280\u001b[39m, in \u001b[36m_fetch_inference_provider_mapping\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03mFetch provider mappings for a model from the Hub.\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhf_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HfApi\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m info = \u001b[43mHfApi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minferenceProviderMapping\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m provider_mapping = info.inference_provider_mapping\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider_mapping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ravik\\OneDrive\\Desktop\\Agentic_Ai_Session\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ravik\\OneDrive\\Desktop\\Agentic_Ai_Session\\venv\\Lib\\site-packages\\huggingface_hub\\hf_api.py:2638\u001b[39m, in \u001b[36mHfApi.model_info\u001b[39m\u001b[34m(self, repo_id, revision, timeout, securityStatus, files_metadata, expand, token)\u001b[39m\n\u001b[32m   2636\u001b[39m     params[\u001b[33m\"\u001b[39m\u001b[33mexpand\u001b[39m\u001b[33m\"\u001b[39m] = expand\n\u001b[32m   2637\u001b[39m r = get_session().get(path, headers=headers, timeout=timeout, params=params)\n\u001b[32m-> \u001b[39m\u001b[32m2638\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2639\u001b[39m data = r.json()\n\u001b[32m   2640\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ModelInfo(**data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ravik\\OneDrive\\Desktop\\Agentic_Ai_Session\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:482\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/mistralai/Mistral-7B-Instruct-v0.3?expand=inferenceProviderMapping (Request ID: Root=1-68b1f472-34a312c41958c5f36a6619d6;f4481049-c0f3-4472-8610-f3190bc0cb14)\n\nInvalid credentials in Authorization header"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()  # Not needed since we set HF_TOKEN above\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "MODEL_ID='mistralai/Mistral-7B-Instruct-v0.3'\n",
    "\n",
    "client=InferenceClient(api_key=os.environ[\"HF_TOKEN\"],model=MODEL_ID)\n",
    "def extract_text(resp):\n",
    "  try:\n",
    "    return resp.choices[0].message.content\n",
    "  except Exception:\n",
    "    try:\n",
    "      return resp['choices'][0]['message']['content']\n",
    "    except:\n",
    "      return str(resp)\n",
    "\n",
    "message=[\n",
    "    {\"role\":\"system\",'content':\"You are a helpful Leetcode Problem Solver\"},\n",
    "    {\"role\":\"user\",\"content\":\"Give me Add Two Numbers Problem solution\"}\n",
    "]\n",
    "try:\n",
    "    resp=client.chat_completion(messages=message,max_tokens=400)\n",
    "    print(extract_text(resp))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21aac715",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HfHubHTTPError' from 'huggingface_hub' (c:\\Users\\ravik\\OneDrive\\Desktop\\Agentic_Ai_Session\\venv\\Lib\\site-packages\\huggingface_hub\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InferenceClient, HfHubHTTPError\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Use token from environment variable\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'HfHubHTTPError' from 'huggingface_hub' (c:\\Users\\ravik\\OneDrive\\Desktop\\Agentic_Ai_Session\\venv\\Lib\\site-packages\\huggingface_hub\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient, HfHubHTTPError\n",
    "import sys\n",
    "\n",
    "# Use token from environment variable\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if hf_token is None:\n",
    "    print(\"Error: HF_TOKEN not found in environment variables.\", file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"HF_TOKEN loaded successfully.\")\n",
    "print(\"Attempting to connect to the Inference API...\")\n",
    "\n",
    "# Initialize the client with a long timeout\n",
    "client = InferenceClient(model=\"mistralai/Mistral-7B-Instruct-v0.3\", token=hf_token, timeout=180)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful Leetcode Problem Solver\"},\n",
    "    {\"role\": \"user\", \"content\": \"Give me Add Two Numbers Problem solution\"}\n",
    "]\n",
    "\n",
    "try:\n",
    "    resp = client.chat_completion(messages=messages, max_tokens=400)\n",
    "    if resp and resp.choices and resp.choices[0] and resp.choices[0].message and resp.choices[0].message.content:\n",
    "        print(\"\\nAPI call was successful. Here is the output:\\n\")\n",
    "        print(resp.choices[0].message.content)\n",
    "    else:\n",
    "        print(\"\\nAPI call succeeded but returned an empty or malformed response.\")\n",
    "        print(f\"Full response object: {resp}\")\n",
    "except HfHubHTTPError as e:\n",
    "    print(\"\\nAPI call failed with an HTTP error:\", file=sys.stderr)\n",
    "    print(e, file=sys.stderr)\n",
    "    print(\"Please check your token and the model's availability.\", file=sys.stderr)\n",
    "except Exception as e:\n",
    "    print(\"\\nAn unexpected error occurred:\", file=sys.stderr)\n",
    "    print(e, file=sys.stderr)\n",
    "print(\"\\nScript finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f0e861",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HfHubHTTPError' from 'huggingface_hub' (c:\\Users\\ravik\\OneDrive\\Desktop\\Agentic_Ai_Session\\venv\\Lib\\site-packages\\huggingface_hub\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InferenceClient, HfHubHTTPError\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m login\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# YOUR HUGGING FACE TOKEN\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# It's best practice to use an environment variable, but for direct testing,\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# we can put it here temporarily. REMOVE IT BEFORE SHARING YOUR CODE.\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'HfHubHTTPError' from 'huggingface_hub' (c:\\Users\\ravik\\OneDrive\\Desktop\\Agentic_Ai_Session\\venv\\Lib\\site-packages\\huggingface_hub\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from huggingface_hub import InferenceClient, HfHubHTTPError\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Use token from environment variable\n",
    "MY_HF_TOKEN = os.environ.get('HF_TOKEN')\n",
    "\n",
    "print(\"Attempting to log in to Hugging Face Hub...\")\n",
    "try:\n",
    "    login(token=MY_HF_TOKEN)\n",
    "    print(\"Login successful! Token has been saved.\")\n",
    "except Exception as e:\n",
    "    print(\"\\nError during Hugging Face login:\", file=sys.stderr)\n",
    "    print(e, file=sys.stderr)\n",
    "    print(\"Please check your token validity.\", file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "\n",
    "client = InferenceClient(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    timeout=300\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful Leetcode Problem Solver\"},\n",
    "    {\"role\": \"user\", \"content\": \"Give me Add Two Numbers Problem solution\"}\n",
    "]\n",
    "\n",
    "print(\"\\nAttempting to call the chat completion API...\")\n",
    "try:\n",
    "    resp = client.chat_completion(\n",
    "        messages=messages,\n",
    "        max_tokens=400\n",
    "    )\n",
    "    if resp and resp.choices and resp.choices[0] and resp.choices[0].message and resp.choices[0].message.content:\n",
    "        print(\"\\nAPI call was successful. Here is the output:\\n\")\n",
    "        print(\"=\"*50)\n",
    "        print(resp.choices[0].message.content)\n",
    "        print(\"=\"*50)\n",
    "    else:\n",
    "        print(\"\\nAPI call succeeded but returned an empty or malformed response.\")\n",
    "        print(f\"Full response object: {resp}\")\n",
    "except HfHubHTTPError as e:\n",
    "    print(\"\\nAPI call failed with an HTTP error:\", file=sys.stderr)\n",
    "    print(\"This is likely due to an invalid token or permissions.\", file=sys.stderr)\n",
    "    print(e, file=sys.stderr)\n",
    "except Exception as e:\n",
    "    print(\"\\nAn unexpected error occurred:\", file=sys.stderr)\n",
    "    print(e, file=sys.stderr)\n",
    "print(\"\\nScript finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1d36263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "API call failed with an error:\n",
      "API call failed with exception: \n",
      "Please check your token, model id, or permissions.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "prompt = \"Write a Python function to add two numbers.\"\n",
    "\n",
    "try:\n",
    "    response = client.text_generation(prompt=prompt, max_new_tokens=64)\n",
    "    print(\"Generated code:\\n\")\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(\"\\nAPI call failed with an error:\", file=sys.stderr)\n",
    "    print(f\"API call failed with exception: {e}\", file=sys.stderr)\n",
    "    print(\"Please check your token, model id, or permissions.\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea07e33c",
   "metadata": {},
   "source": [
    "Your last code cell attempts to use the `client.text_generation` method with a `prompt` variable, but the variable `prompt` is not defined anywhere in the notebook before this cell. This will cause a NameError.\n",
    "\n",
    "To fix this, you need to define the `prompt` variable before calling `client.text_generation`. For example, add a line like:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9024fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a Python function to add two numbers.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90066f6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "right before the try-except block. Would you like me to add this fix for you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858e7944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Below inference api\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3793774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN'] \n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "MODEL_ID = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
    "client = InferenceClient(api_key=os.environ['HF_TOKEN'], model=MODEL_ID)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful Leetcode Problem Solver\"},\n",
    "    {\"role\": \"user\", \"content\": \"Give me Add Two Numbers Problem solution\"}\n",
    "]\n",
    "try:\n",
    "    resp = client.chat_completion(messages=messages, max_tokens=400)\n",
    "    print(resp.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(f\"Chat completion error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813424a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
