{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2689f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from groq import Groq\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain.schema import Document\n",
    "from datetime import datetime,timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69594f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(api_key = os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "371581e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAILABLE_MODELS = [\n",
    "    \"allam-2-7b\",\n",
    "    \"compound-beta\",\n",
    "    \"compound-beta-mini\",\n",
    "    \"deepseek-r1-distill-llama-70b\",\n",
    "    \"gemma2-9b-it\",\n",
    "    \"llama-3.1-8b-instant\",\n",
    "    \"llama-3.3-70b-versatile\",\n",
    "    \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "    \"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    \"meta-llama/llama-guard-4-12b\",\n",
    "    \"meta-llama/llama-prompt-guard-2-22m\",\n",
    "    \"meta-llama/llama-prompt-guard-2-86m\",\n",
    "    \"moonshotai/kimi-k2-instruct\",\n",
    "    \"openai/gpt-oss-120b\",\n",
    "    \"openai/gpt-oss-20b\",\n",
    "    \"qwen/qwen3-32b\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cdb3bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Content For : Machine Learning\n",
      "Loaded Content For : Deep Learning\n",
      "Loaded Content For : Data Science\n",
      "Loaded Content For : Explainable Ai\n",
      "Loaded Content For : Agentic AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_22200\\1206283098.py:18: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base Created successfully\n"
     ]
    }
   ],
   "source": [
    "def create_knowledge_base():\n",
    "    topics = [\"Machine Learning\",\"Deep Learning\",\"Data Science\",\"Explainable Ai\",\"Agentic AI\"]\n",
    "    documents = []\n",
    "    \n",
    "    for topic in topics:\n",
    "        try:\n",
    "            loader = WikipediaLoader(query = topic,load_max_docs = 1)\n",
    "            docs = loader.load()\n",
    "            documents.extend(docs)\n",
    "            print(f\"Loaded Content For : {topic}\")\n",
    "        except Exception as e:\n",
    "            print(f\"could not load content for {topic } : {e}\")\n",
    "            fallback_content = f\"{topic} is a fascinating subject that involves systematic content Tell no data available for the given topic \"\n",
    "            documents.append(Document(page_content = fallback_content))\n",
    "            \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000 , chunk_overlap = 200)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
    "    knowledge_base = Chroma.from_documents(texts,embeddings)\n",
    "    print(\"Knowledge base Created successfully\")\n",
    "    return knowledge_base    \n",
    "knowledge_base = create_knowledge_base()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f429e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_educational_content(query  :str)-> str:\n",
    "    try : \n",
    "        docs = knowledge_base.similarity_search(query,k = 3)\n",
    "        content = \"\\n\\n\".join([f\"Source {i+1} :\\n {doc.page_content[:500]}...\" for i,doc in enumerate(docs)])\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        return f\"Information about {query} : Error retrieving content - {str(e)}\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4b7a32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_quiz(topic: str, difficulty: str = \"Beginner\", model: str = \"deepseek-r1-distill-llama-70b\") -> str:\n",
    "  \"\"\"Generate a quiz on a specific topic.\"\"\"\n",
    "  prompt = f\"\"\"Create a {difficulty} level quiz about {topic} with 3 questions.\n",
    "  Format i clearly with:\n",
    "  1. Question 1\n",
    "  2. Question 2\n",
    "  3. Question 3\n",
    "  Then provide answers at the end.\n",
    "  \"\"\"\n",
    "  \n",
    "  try:\n",
    "      chat_completion = client.chat.completions.create(\n",
    "          messages = [{\"role\":\"user\",\"content\" : prompt}],\n",
    "          model = model,\n",
    "          max_tokens = 500)\n",
    "      return chat_completion.choices[0].message.content\n",
    "  except Exception as e:\n",
    "      return f\"could not generate quiz : {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67bc4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_session(topic  :str,student_level : str,model : str)->str:\n",
    "    prompt = f\"\"\"\n",
    "    As an AI scheduling assistant, created a personlized study for learning {topic} at\n",
    "    {student_level} level . \n",
    "    Include:\n",
    "    -Recommended session times\n",
    "    - Duration for each session\n",
    "    -topic to cover in each session\n",
    "    - Breaks and review periods\n",
    "    \"\"\"\n",
    "    \n",
    "    try: \n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages = [{\"role\" : \"user\",\"content\" : prompt}],\n",
    "            model = model,\n",
    "            max_tokens = 300\n",
    "        )\n",
    "        schedule_content = chat_completion.choices[0].message.content\n",
    "        session_time = (datetime.now() + timedelta(hours = 24)).strftime(\"%Y-%m-%d at %H:%M\" )\n",
    "        return f\"{schedule_content}\\n\\nFirst session scheduled on '{topic}' for {student_level} level at {session_time} .Remainder set!\"\n",
    "    except Exception as e:\n",
    "        session_time = (datetime.now() + timedelta(hours=24)).strftime(\"%Y-%m-%d at %H:%M\")\n",
    "        return f\"scheduled session on '{topic}' for {student_level} level at {session_time}.Remainder set!\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fc3ab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_concept(concept : str , student_level :str , model : str = \"deepseek-r1-distill-llama-70b\") -> str:\n",
    "    prompt = f\"\"\" \n",
    "    Explain a comncept inm simple terms with examples.\n",
    "    Explain {concept} in simple terms for a  {student_level} level student  .\n",
    "    Use clear language, practical example , and keep it under 300 words.\n",
    "    structure you explanation with :\n",
    "    - Basic definition\n",
    "    - key concepts\n",
    "    - Real-world exapmles \n",
    "    - Why it's important\n",
    "    \n",
    "    \"\"\"\n",
    "    try : \n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages = [{\"role\" : \"user\" , \"content\" : prompt}],\n",
    "            model = model,\n",
    "            max_tokens = 500\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Could not generate a explanation : {str(e)}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abaf6e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_feedback(topic : str ,student_responses : str ,  student : str ,student_level : str ,model : str = \"deepseek-r1-distill-llama-70b\") ->str :\n",
    "    if not student_responses.strip():\n",
    "        return \"No student responses provided for feedback\"\n",
    "    prompt = f\"\"\" Provide constructive feedback on these { student_level} level student responses about {topic} : \n",
    "    \n",
    "    student Answers :\n",
    "    {student_responses}\n",
    "    \n",
    "    provide:\n",
    "    1. Positive renforcement\n",
    "    2.specific areas for imporovemet\n",
    "    3.Suggestions for better understanding\n",
    "    4.Encouragement for continued learning\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages = [{\"role\" : \"user\" , \"content\"  :prompt}],\n",
    "            model = model,\n",
    "            max_tokes = 300\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"could not generate  feedback : {str(e)}\"\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27aa366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_learning_path(topic: str, student_level: str, difficulty: str, model: str = \"deepseek-r1-distill-llama-70b\") -> str:\n",
    "\n",
    "  \"\"\"Create a personalized learning path for the student.\"\"\"\n",
    "  prompt = f\"\"\"\n",
    "  Create a step by learning path \n",
    "  Include : \n",
    "  - Learnig objectives\n",
    "  - Recommend study\n",
    "  - key milestones\n",
    "  - suggested resources\n",
    "  - practice excercises\n",
    "  \"\"\" \n",
    "\n",
    "  try:\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages =[{\"role\":\"user\", \"content\":prompt}],\n",
    "        model = model,\n",
    "        max_tokens = 400\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "  except Exception as e:\n",
    "    return f\"Could not generate learning path: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9185fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified tutoring function\n",
    "def simplified_tutoring(topic , student_level , difficulty , student_responses , model):\n",
    "  \"\"\"Main tutoring function that coordinates all agents\"\"\"\n",
    "  results = []\n",
    "  results.append(\"Multi-Agent Tutoring System Initialized * \")\n",
    "  results.append(f\"Topic: {topic}\")\n",
    "  results.append(f\"level: * {student_level}\")\n",
    "  results.append(f\"Difficulty: * {difficulty}\")\n",
    "  results.append(f\"* Ai Model * {model}\")\n",
    "  results.append(\"=\"*60)\n",
    "    \n",
    "  results.append(\"\\n *Lead TUTOTR : Creating Learning Path *\")\n",
    "  learning_path = create_learning_path(topic,student_level,difficulty,model)\n",
    "  results.append(learning_path)\n",
    "\n",
    "  results.append(\"\\n*Content RETRIEVER: findind respurces\")\n",
    "  content = retrive_educational_content(topic)\n",
    "  results.append(content)\n",
    "\n",
    "  results.append(\"\\n*CONCEPT EXPLAINER:Clear Explanation*\")\n",
    "  explanation=explain_concept(topic, student_level,model)\n",
    "  results.append(explanation)\n",
    "\n",
    "  results.append(\"\\n*QUIZ GENERATOR:Generating Quiz*\")\n",
    "  quiz = generate_quiz(topic,difficulty,model)\n",
    "  results.append(quiz)\n",
    "\n",
    "  results.append(\"\\n*SESSION SCHEDULER:Scheduling Session*\")\n",
    "  session_schedule = schedule_session(topic,student_level,model)\n",
    "  results.append(session_schedule)\n",
    "\n",
    "  results.append(\"\\n*FEEDBACK GENERATOR:Provide Feedback*\")\n",
    "  feedback = provide_feedback(topic,student_responses,student_level,model)\n",
    "  results.append(feedback)\n",
    "\n",
    "  results.append(\"\\n\"+\"=\"*60)\n",
    "  results.append(\"Tutoring session completed\")\n",
    "  return \"\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a124ab68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting AI Tutoring System...\n",
      "Available models: ['allam-2-7b', 'compound-beta', 'compound-beta-mini', 'deepseek-r1-distill-llama-70b', 'gemma2-9b-it', 'llama-3.1-8b-instant', 'llama-3.3-70b-versatile', 'meta-llama/llama-4-maverick-17b-128e-instruct', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-guard-4-12b', 'meta-llama/llama-prompt-guard-2-22m', 'meta-llama/llama-prompt-guard-2-86m', 'moonshotai/kimi-k2-instruct', 'openai/gpt-oss-120b', 'openai/gpt-oss-20b', 'qwen/qwen3-32b']\n",
      "Groq API connection successful!\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/03 12:25:29 [W] [service.go:132] login to server failed: dial tcp 44.237.78.176:7000: i/o timeout\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting tutoring session: Machine Learning, Intermediate, Hard, openai/gpt-oss-20b\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> None\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "def tutoring_interface(topic, student_level, difficulty, student_responses, model):\n",
    "    try:\n",
    "        # Validate inputs\n",
    "        if not topic.strip():\n",
    "            return \"Please enter a topic to learn about!\"\n",
    "\n",
    "\n",
    "        print(f\"Starting tutoring session: {topic}, {student_level}, {difficulty}, {model}\")\n",
    "        result = simplified_tutoring(topic, student_level, difficulty, student_responses, model)\n",
    "        return result\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error in tutoring system: {str(e)}\"\n",
    "        error_msg += \"\\n\\n Troubleshooting tips:\"\n",
    "        error_msg += \"\\n1. Check your Groq API key is valid and set\"\n",
    "        error_msg += \"\\n2. Ensure you have internet connection\"\n",
    "        error_msg += \"\\n3. Try a different topic or simpler query\"\n",
    "        error_msg += \"\\n4. Some models might not be available in your region\"\n",
    "        return error_msg\n",
    "\n",
    "\n",
    "# Set up the Gradio interface\n",
    "with gr.Blocks(title=\"AI Tutoring System\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    #  Multi-Agent AI Tutoring System\n",
    "    Get personalized tutoring across various subjects with our AI-powered multi-agent system\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"###  Learning Preferences\")\n",
    "            topic = gr.Textbox(\n",
    "                label=\"What would you like to learn about?\",\n",
    "                placeholder=\"E.g., Quantum Physics, Calculus, Machine Learning...\",\n",
    "                value=\"Quantum Mechanics\"\n",
    "            )\n",
    "            student_level = gr.Dropdown(\n",
    "                [\"Beginner\", \"Intermediate\", \"Advanced\"],\n",
    "                label=\"Your current level\",\n",
    "                value=\"Beginner\"\n",
    "            )\n",
    "            difficulty = gr.Dropdown(\n",
    "                [\"Easy\", \"Medium\", \"Hard\"],\n",
    "                label=\"Desired difficulty level\",\n",
    "                value=\"Medium\"\n",
    "            )\n",
    "            model = gr.Dropdown(\n",
    "                AVAILABLE_MODELS,\n",
    "                label=\"Select AI Model\",\n",
    "                value=\"deepseek-r1-distill-llama-70b\",\n",
    "                info=\"Choose which model to use for content generation\"\n",
    "            )\n",
    "            student_responses = gr.Textbox(\n",
    "                label=\"Your previous answers (for feedback)\",\n",
    "                lines=3,\n",
    "                value=\"\",\n",
    "                placeholder=\"Paste your quiz answers here for personalized feedback...\"\n",
    "            )\n",
    "            submit_btn = gr.Button(\" Start Learning Session\", variant=\"primary\")\n",
    "\n",
    "\n",
    "            gr.Markdown(\"###  Tips\")\n",
    "            gr.Markdown(\"\"\"\n",
    "            - Be specific with your topic\n",
    "            - Choose the level that matches your knowledge\n",
    "            - Use the feedback section to get personalized tips\n",
    "            - Different models may provide different response styles\n",
    "            - Larger models (like 70B) generally provide more detailed responses\n",
    "            \"\"\")\n",
    "\n",
    "\n",
    "        with gr.Column(scale=2):\n",
    "            gr.Markdown(\"###  Tutoring Session Output\")\n",
    "            output = gr.Textbox(\n",
    "                label=\"Session Results\",\n",
    "                lines=25,\n",
    "                interactive=False,\n",
    "                show_copy_button=True\n",
    "            )\n",
    "\n",
    "\n",
    "    # Examples\n",
    "    gr.Markdown(\"###  Example Learning Topics\")\n",
    "    examples = gr.Examples(\n",
    "        examples=[\n",
    "            [\"Quantum Physics\", \"Beginner\", \"Easy\", \"\", \"deepseek-r1-distill-llama-70b\"],\n",
    "            [\"Machine Learning\", \"Intermediate\", \"Medium\", \"\", \"llama-3.3-70b-versatile\"],\n",
    "            [\"Calculus\", \"Intermediate\", \"Medium\", \"\", \"gemma2-9b-it\"],\n",
    "            [\"Python Programming\", \"Beginner\", \"Easy\", \"\", \"llama-3.1-8b-instant\"],\n",
    "            [\"Neural Networks\", \"Advanced\", \"Hard\", \"\", \"qwen/qwen3-32b\"]\n",
    "        ],\n",
    "        inputs=[topic, student_level, difficulty, student_responses, model],\n",
    "        label=\"Click any example to load it\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # Connect the button\n",
    "    submit_btn.click(\n",
    "        fn=tutoring_interface,\n",
    "        inputs=[topic, student_level, difficulty, student_responses, model],\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "\n",
    "# Launch the application\n",
    "if __name__== \"__main__\":\n",
    "    print(\"Starting AI Tutoring System...\")\n",
    "    print(\"Available models:\", AVAILABLE_MODELS)\n",
    "\n",
    "\n",
    "    # Test the API key\n",
    "    try:\n",
    "        test_response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "            model=\"deepseek-r1-distill-llama-70b\",\n",
    "            max_tokens=10\n",
    "        )\n",
    "        print(\"Groq API connection successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Groq API error: {e}\")\n",
    "        print(\"Please check your API key and try again.\")\n",
    "\n",
    "\n",
    "    demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946e9c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
