{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a7916db",
   "metadata": {},
   "source": [
    "# SERP_API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e54c2eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 35.3 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.5/12.8 MB 36.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 22.3 MB/s  0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import networkx as nx\n",
    "! python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a18031e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# skill = input(\"Ask any job to search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a43104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Staus code 401\n",
      "No jobs found . Check query or api usage limits \n",
      "No job listings returned\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "def fetch_jobs_google(skill):\n",
    "    params = {\n",
    "        \"engine\" : \"google_jobs\",\n",
    "        \"q\" : f\"{skill} jobs in Coimbatore\",\n",
    "        \"hl\": \"en\",\n",
    "        \"api_key\" : os.getenv(\"SERP_API_KEY\")\n",
    "        \n",
    "    }\n",
    "    \n",
    "    response = requests.get(\"https://serpapi.com/search\",params = params)\n",
    "    print(\"API Staus code\",response.status_code)\n",
    "    \n",
    "    data = response.json()\n",
    "    jobs = data.get(\"jobs_results\",[])\n",
    "    \n",
    "    if not jobs:\n",
    "        print(\"No jobs found . Check query or api usage limits \")\n",
    "        return pd.DataFrame()\n",
    "    return pd.DataFrame([{\n",
    "        \"title\":job.get(\"title\"),\n",
    "        \"company\":job.get(\"company_name\"),\n",
    "        \"location\":job.get(\"location\"),\n",
    "        \"via\":job.get(\"via\"),\n",
    "        \"url\":job.get(\"apply_options\",[{}])[0].get(\"link\",\"\") if job.get(\"apply_options\")else \"\",\n",
    "        \n",
    "    }for job in jobs])\n",
    "    \n",
    "df_jobs = fetch_jobs_google(skill)\n",
    "if not df_jobs.empty:\n",
    "    print(df_jobs[['title','company','location','url']].head())\n",
    "else:\n",
    "    print(\"No job listings returned\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb42bd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastapi import FastAPI\n",
    "# import uvicorn,nest_asyncio\n",
    "# from pyngrok import ngrok\n",
    "\n",
    "# app = FastAPI()\n",
    "\n",
    "# @app.get(\"/health\")\n",
    "# def health():\n",
    "#     return {\"status\":\"Good\",\"version\":\"1.0.112\"}\n",
    "\n",
    "\n",
    "# nest_asyncio.apply()\n",
    "# ngrok.set_auth_token(os.getenv(\"NGROK_API_KEY\"))\n",
    "# public_url = ngrok.connect(8000)\n",
    "# print(\"public URL :\",public_url)\n",
    "# uvicorn.run(app,host = \"0.0.0.0\",port = 8000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cf066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "x = np.array([[1],[2],[3]])\n",
    "y = np.array([2,4,6])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x,y)\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "class Input(BaseModel):\n",
    "    x:float\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "\n",
    "def predict(data:Input):\n",
    "    prediction = model.predict(np.array([[data.x]]))[0]\n",
    "    return {\"prediction\":prediction}\n",
    "\n",
    "nest_asyncio.apply()\n",
    "ngrok.set_auth_token(os.getenv(\"NGROK_API_KEY\"))\n",
    "\n",
    "public_url = ngrok.connect(8000)\n",
    "\n",
    "print(\"public_url\",public_url)\n",
    "\n",
    "uvicorn.run(app,host = \"0.0.0.0\",port = 8000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba459c6",
   "metadata": {},
   "source": [
    "# AUTOGEN ->\n",
    "\n",
    "1.Assistant agent -> it is the main working agent\n",
    "2.user proxy agent -> it have the constraints that the user do have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb0cea22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac6b0e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from autogen import AssistantAgent , UserProxyAgent\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c2a569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to groq_assistant):\n",
      "\n",
      "What is Current Trend in the AI field ? \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mgroq_assistant\u001b[0m (to user_proxy):\n",
      "\n",
      "The current trends in the AI field are exciting and rapidly evolving. Here are some of the most significant ones:\n",
      "\n",
      "1. **Explainable AI (XAI)**: As AI becomes more pervasive, there is a growing need to understand how AI systems make decisions. XAI focuses on developing techniques to explain and interpret AI-driven decisions.\n",
      "2. **Deep Learning**: Deep learning techniques, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), continue to dominate the field. These methods have achieved state-of-the-art results in image and speech recognition, natural language processing, and more.\n",
      "3. **Natural Language Processing (NLP)**: NLP has seen significant advancements in recent years, with the development of transformer-based models like BERT, RoBERTa, and XLNet. These models have improved language understanding, sentiment analysis, and text generation capabilities.\n",
      "4. **Computer Vision**: Computer vision has made tremendous progress, with applications in self-driving cars, facial recognition, and medical imaging. Techniques like object detection, segmentation, and image generation are becoming increasingly sophisticated.\n",
      "5. **Edge AI**: With the proliferation of IoT devices, edge AI is gaining traction. Edge AI involves running AI models on edge devices, reducing latency, and improving real-time decision-making.\n",
      "6. **Transfer Learning**: Transfer learning has become a crucial aspect of AI development, allowing models to leverage pre-trained knowledge and fine-tune them for specific tasks.\n",
      "7. **AutoML (Automated Machine Learning)**: AutoML aims to automate the machine learning process, making it more accessible to non-experts. This trend is driven by the need for faster and more efficient model development.\n",
      "8. **Adversarial Robustness**: As AI systems become more widespread, adversarial attacks have become a significant concern. Adversarial robustness involves developing techniques to defend AI models against such attacks.\n",
      "9. **Multimodal Learning**: Multimodal learning involves combining multiple sources of data, such as text, images, and audio, to improve AI model performance and understanding.\n",
      "10. **Ethics and Fairness**: As AI becomes more pervasive, there is a growing need to address concerns around ethics, fairness, and bias in AI systems. Researchers are working to develop more transparent, accountable, and fair AI models.\n",
      "11. **Reinforcement Learning**: Reinforcement learning has seen significant advancements, with applications in robotics, game playing, and autonomous systems.\n",
      "12. **Quantum AI**: The intersection of AI and quantum computing is an emerging area of research, with potential applications in cryptography, optimization, and machine learning.\n",
      "\n",
      "These trends are not mutually exclusive, and many are interconnected. As AI continues to evolve, we can expect to see even more innovative applications and breakthroughs in the field.\n",
      "\n",
      "Would you like to know more about any specific trend?\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ravik\\OneDrive\\Desktop\\Agentic_Ai_Session\\venv\\Lib\\site-packages\\autogen\\oai\\groq.py:315: UserWarning: Cost calculation not available for model llama-3.3-70b-versatile\n",
      "  warnings.warn(f\"Cost calculation not available for model {model}\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to groq_assistant):\n",
      "\n",
      "what about Agentic Ai\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mgroq_assistant\u001b[0m (to user_proxy):\n",
      "\n",
      "**Agentic AI** is a fascinating and rapidly evolving area of research. Agentic AI refers to AI systems that can perceive their environment, make decisions, and act autonomously to achieve their goals. These systems are designed to be proactive, adaptive, and interactive, often learning from their experiences and improving their performance over time.\n",
      "\n",
      "Key characteristics of Agentic AI include:\n",
      "\n",
      "1. **Autonomy**: Agentic AI systems operate independently, making decisions and taking actions without explicit human intervention.\n",
      "2. **Perception**: They can perceive their environment through various sensors, such as cameras, microphones, or lidar.\n",
      "3. **Reasoning**: Agentic AI systems use reasoning and decision-making algorithms to interpret their perceptions and make informed decisions.\n",
      "4. **Action**: They can take actions in their environment, such as moving, manipulating objects, or communicating with humans.\n",
      "5. **Learning**: Agentic AI systems can learn from their experiences, adapting to new situations and improving their performance over time.\n",
      "\n",
      "Agentic AI has many potential applications, including:\n",
      "\n",
      "1. **Robotics**: Agentic AI can be used to control robots that can navigate, manipulate objects, and interact with humans.\n",
      "2. **Autonomous vehicles**: Self-driving cars and drones can use Agentic AI to perceive their environment, make decisions, and take actions to ensure safe and efficient navigation.\n",
      "3. **Smart homes and cities**: Agentic AI can be used to control and optimize smart home and city systems, such as energy management, traffic flow, and waste management.\n",
      "4. **Healthcare**: Agentic AI can be used to develop personalized healthcare systems that can monitor patients, make decisions, and take actions to improve their health and well-being.\n",
      "5. **Cybersecurity**: Agentic AI can be used to detect and respond to cyber threats in real-time, improving the security and resilience of computer systems and networks.\n",
      "\n",
      "Some of the benefits of Agentic AI include:\n",
      "\n",
      "1. **Increased efficiency**: Agentic AI systems can automate routine tasks, freeing humans to focus on more complex and creative work.\n",
      "2. **Improved safety**: Agentic AI systems can detect and respond to potential hazards, reducing the risk of accidents and injuries.\n",
      "3. **Enhanced decision-making**: Agentic AI systems can provide real-time insights and recommendations, helping humans make better decisions.\n",
      "4. **Personalization**: Agentic AI systems can learn individual preferences and adapt to unique situations, providing personalized experiences and services.\n",
      "\n",
      "However, Agentic AI also raises important questions and concerns, such as:\n",
      "\n",
      "1. **Accountability**: Who is responsible when an Agentic AI system makes a mistake or causes harm?\n",
      "2. **Transparency**: How can we ensure that Agentic AI systems are transparent and explainable in their decision-making processes?\n",
      "3. **Security**: How can we protect Agentic AI systems from cyber threats and ensure their reliability and integrity?\n",
      "4. **Ethics**: How can we ensure that Agentic AI systems align with human values and ethics, and do not perpetuate biases or discrimination?\n",
      "\n",
      "To address these challenges, researchers and developers are working on creating Agentic AI systems that are:\n",
      "\n",
      "1. **Explainable**: Providing insights into their decision-making processes and actions.\n",
      "2. **Transparent**: Allowing humans to understand and trust their operations.\n",
      "3. **Accountable**: Ensuring that Agentic AI systems are responsible for their actions and can be held accountable for any mistakes or harm.\n",
      "4. **Value-aligned**: Designing Agentic AI systems that align with human values and ethics, and promote beneficial outcomes.\n",
      "\n",
      "The development of Agentic AI is an active area of research, with many opportunities for innovation and advancement. As Agentic AI continues to evolve, we can expect to see significant advancements in areas like robotics, autonomous systems, and human-computer interaction.\n",
      "\n",
      "Would you like to know more about Agentic AI or its applications?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (2fdeaaad-e2c8-4dee-84ce-bd2d52fba465): User requested to end the conversation\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "configure_list = [ {\n",
    "    \"model\" : \"llama-3.3-70b-versatile\",\n",
    "    \"api_key\" : os.getenv(\"GROQ_API_KEY\"),\n",
    "    \"api_type\" : \"groq\"\n",
    "}]\n",
    "\n",
    "assistant = AssistantAgent(\n",
    "    name = \"groq_assistant\",\n",
    "    system_message = \"You area a helpful AI Assistant\",\n",
    "    llm_config = {\"config_list\":configure_list}\n",
    ")\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "    name = \"user_proxy\",\n",
    "    code_execution_config = False,\n",
    "    \n",
    ")\n",
    "\n",
    "user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message = \"What is Current Trend in the AI field ? \")\n",
    "\n",
    "work_dir = Path(\"coding\")\n",
    "work_dir.mkdir(exist_ok = True)\n",
    "code_executor = LocalCommandLineCodeExecutor(work_dir=work_dir)\n",
    "\n",
    "user_proxy_with_code = UserProxyAgent(\n",
    "    name = \"user_proxy_with_code\",\n",
    "    code_execution_config={\"executor\":code_executor}\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0459439",
   "metadata": {},
   "source": [
    "# Auto Gen - Two agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4549fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to assitant1):\n",
      "\n",
      "Explain machine learning to me in simple terms.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massitant1\u001b[0m (to user):\n",
      "\n",
      "**Machine Learning in Simple Terms**\n",
      "\n",
      "Machine learning is a way to teach computers to make decisions on their own, without being explicitly programmed. Here's how it works:\n",
      "\n",
      "1. **You give the computer data**: This can be pictures, words, numbers, or any other type of information.\n",
      "2. **The computer looks for patterns**: It tries to find relationships between the data, like \"when it's sunny, people wear shorts\".\n",
      "3. **The computer makes predictions**: Based on the patterns it found, the computer makes predictions about new, unseen data.\n",
      "4. **The computer learns from its mistakes**: When the computer makes a mistake, it uses that information to improve its predictions next time.\n",
      "\n",
      "**Example:** Imagine you want to teach a computer to recognize pictures of dogs and cats. You show it many pictures of dogs and cats, and it looks for patterns in the data. After a while, it can look at a new picture and say \"oh, that's a dog!\" or \"that's a cat!\".\n",
      "\n",
      "**Key Idea:** Machine learning is like teaching a child to recognize objects. You show them many examples, and they start to understand the patterns and relationships between things. The computer does the same thing, but with data instead of objects.\n",
      "\n",
      "That's machine learning in simple terms!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ravik\\OneDrive\\Desktop\\Agentic_Ai_Session\\venv\\Lib\\site-packages\\autogen\\oai\\groq.py:315: UserWarning: Cost calculation not available for model llama-3.3-70b-versatile\n",
      "  warnings.warn(f\"Cost calculation not available for model {model}\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33muser\u001b[0m (to assitant1):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massitant1\u001b[0m (to user):\n",
      "\n",
      "It seems like you didn't ask a question or provide a topic for me to explain in simple terms. Please feel free to ask me anything, and I'll do my best to explain it in a clear and concise manner.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser\u001b[0m (to assitant1):\n",
      "\n",
      "auto-reply\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massitant1\u001b[0m (to user):\n",
      "\n",
      "**Auto-Reply in Simple Terms**\n",
      "\n",
      "An auto-reply is a message that is sent automatically in response to an email, text, or other type of communication. It's like a pre-written message that says \"I'm not available right now, but I'll get back to you soon\".\n",
      "\n",
      "**How it works:**\n",
      "\n",
      "1. **You set up an auto-reply**: You write a message that will be sent automatically when someone contacts you.\n",
      "2. **Someone sends you a message**: When someone emails or texts you, the auto-reply system kicks in.\n",
      "3. **The auto-reply is sent**: The pre-written message is sent back to the person who contacted you.\n",
      "\n",
      "**Common uses:**\n",
      "\n",
      "* Out-of-office notifications: \"I'm on vacation and will respond when I get back\".\n",
      "* Automated customer support: \"Thank you for contacting us, we'll get back to you soon\".\n",
      "* Appointment reminders: \"Don't forget your appointment tomorrow at 2 PM\".\n",
      "\n",
      "**Key benefit:** Auto-replies help you manage your communications when you're not available, and provide a quick response to people who contact you.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser\u001b[0m (to assitant1):\n",
      "\n",
      "skip\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massitant1\u001b[0m (to user):\n",
      "\n",
      "It looks like you want to skip to something else. No problem! What would you like to learn about or discuss? I'm here to help and explain things in simple terms.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33muser\u001b[0m (to assitant1):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massitant1\u001b[0m (to user):\n",
      "\n",
      "It seems like you didn't type anything. If you're ready to move on, I can suggest some topics or you can ask me a question. What's on your mind?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33muser\u001b[0m (to assitant1):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massitant1\u001b[0m (to user):\n",
      "\n",
      "It looks like you didn't type anything again. If you're ready to chat or ask a question, I'm here to help. Otherwise, feel free to start fresh whenever you're ready!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (e2c01bb3-cb50-4fe4-a9e5-fe66438e0a18): User requested to end the conversation\u001b[0m\n",
      "\u001b[33muser\u001b[0m (to Assistant2):\n",
      "\n",
      "Now give me a detailed technical explanation\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mAssistant2\u001b[0m (to user):\n",
      "\n",
      "Let's dive into a detailed technical explanation of a complex topic: **Convolutional Neural Networks (CNNs) for Image Classification**.\n",
      "\n",
      "**Introduction**\n",
      "\n",
      "Convolutional Neural Networks (CNNs) are a type of deep learning model that have revolutionized the field of computer vision. They are particularly well-suited for image classification tasks, where the goal is to assign a label or category to an input image. In this explanation, we'll delve into the technical details of CNNs and explore how they work.\n",
      "\n",
      "**Architecture Overview**\n",
      "\n",
      "A typical CNN architecture consists of several layers:\n",
      "\n",
      "1. **Convolutional Layers**: These layers apply a set of learnable filters to the input image, scanning the image in a sliding window fashion. Each filter computes a feature map, which represents the presence of a particular feature at different locations in the image.\n",
      "2. **Activation Functions**: The output of the convolutional layer is passed through an activation function, such as ReLU (Rectified Linear Unit) or Sigmoid, to introduce non-linearity into the model.\n",
      "3. **Pooling Layers**: These layers downsample the feature maps, reducing the spatial dimensions while retaining the most important information.\n",
      "4. **Flattening Layers**: The output of the convolutional and pooling layers is flattened into a 1D array, preparing the data for the fully connected layers.\n",
      "5. **Fully Connected Layers**: These layers are used for classification, where the flattened output is fed into a neural network to produce a probability distribution over the classes.\n",
      "\n",
      "**Convolutional Layers**\n",
      "\n",
      "Convolutional layers are the core component of CNNs. They consist of a set of filters, each with a fixed size (e.g., 3x3 or 5x5) and a stride (e.g., 1 or 2). The filters are applied to the input image, scanning it in a sliding window fashion, to compute the feature maps.\n",
      "\n",
      "The convolutional layer can be mathematically represented as:\n",
      "\n",
      "`output = σ(W * input + b)`\n",
      "\n",
      "where:\n",
      "\n",
      "* `output` is the feature map\n",
      "* `W` is the filter (kernel)\n",
      "* `input` is the input image\n",
      "* `b` is the bias term\n",
      "* `σ` is the activation function (e.g., ReLU)\n",
      "\n",
      "The convolutional layer has several key properties:\n",
      "\n",
      "* **Spatial Hierarchies**: The filters are applied at multiple scales, allowing the model to capture features at different levels of abstraction.\n",
      "* **Shared Weights**: The same filter is applied to multiple locations in the image, reducing the number of parameters and improving generalization.\n",
      "* **Local Connectivity**: Each output pixel is computed based on a small region of the input image, preserving spatial relationships.\n",
      "\n",
      "**Backpropagation and Optimization**\n",
      "\n",
      "During training, the CNN model is optimized using backpropagation, which computes the gradient of the loss function with respect to the model's parameters. The optimization algorithm (e.g., Stochastic Gradient Descent, Adam) updates the model's parameters to minimize the loss function.\n",
      "\n",
      "The backpropagation process involves:\n",
      "\n",
      "1. **Forward Pass**: The input image is passed through the network, computing the output and loss.\n",
      "2. **Backward Pass**: The error is propagated backwards, computing the gradients of the loss with respect to each layer's parameters.\n",
      "3. **Weight Update**: The model's parameters are updated based on the gradients and the optimization algorithm.\n",
      "\n",
      "**Convolutional Neural Network Example**\n",
      "\n",
      "Let's consider a simple example of a CNN architecture for image classification:\n",
      "\n",
      "* Input: 28x28 grayscale images\n",
      "* Convolutional Layer 1: 32 filters, 3x3 kernel, stride 1\n",
      "* Activation Function: ReLU\n",
      "* Pooling Layer 1: 2x2 max pooling, stride 2\n",
      "* Convolutional Layer 2: 64 filters, 3x3 kernel, stride 1\n",
      "* Activation Function: ReLU\n",
      "* Pooling Layer 2: 2x2 max pooling, stride 2\n",
      "* Flattening Layer: 128 units\n",
      "* Fully Connected Layer: 10 units (output layer)\n",
      "\n",
      "This example illustrates the basic components of a CNN architecture, including convolutional layers, activation functions, pooling layers, flattening layers, and fully connected layers.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Convolutional Neural Networks (CNNs) are a powerful tool for image classification tasks. By leveraging spatial hierarchies, shared weights, and local connectivity, CNNs can learn complex features and patterns in images. The technical details of CNNs, including convolutional layers, activation functions, pooling layers, and backpropagation, are crucial to understanding how these models work. By grasping these concepts, developers and researchers can design and implement effective CNN architectures for a wide range of computer vision applications.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (38d9eaa1-b7e6-497a-a745-9beaa02fcf94): User requested to end the conversation\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': 'Now give me a detailed technical explanation', 'role': 'assistant', 'name': 'user'}, {'content': \"Let's dive into a detailed technical explanation of a complex topic: **Convolutional Neural Networks (CNNs) for Image Classification**.\\n\\n**Introduction**\\n\\nConvolutional Neural Networks (CNNs) are a type of deep learning model that have revolutionized the field of computer vision. They are particularly well-suited for image classification tasks, where the goal is to assign a label or category to an input image. In this explanation, we'll delve into the technical details of CNNs and explore how they work.\\n\\n**Architecture Overview**\\n\\nA typical CNN architecture consists of several layers:\\n\\n1. **Convolutional Layers**: These layers apply a set of learnable filters to the input image, scanning the image in a sliding window fashion. Each filter computes a feature map, which represents the presence of a particular feature at different locations in the image.\\n2. **Activation Functions**: The output of the convolutional layer is passed through an activation function, such as ReLU (Rectified Linear Unit) or Sigmoid, to introduce non-linearity into the model.\\n3. **Pooling Layers**: These layers downsample the feature maps, reducing the spatial dimensions while retaining the most important information.\\n4. **Flattening Layers**: The output of the convolutional and pooling layers is flattened into a 1D array, preparing the data for the fully connected layers.\\n5. **Fully Connected Layers**: These layers are used for classification, where the flattened output is fed into a neural network to produce a probability distribution over the classes.\\n\\n**Convolutional Layers**\\n\\nConvolutional layers are the core component of CNNs. They consist of a set of filters, each with a fixed size (e.g., 3x3 or 5x5) and a stride (e.g., 1 or 2). The filters are applied to the input image, scanning it in a sliding window fashion, to compute the feature maps.\\n\\nThe convolutional layer can be mathematically represented as:\\n\\n`output = σ(W * input + b)`\\n\\nwhere:\\n\\n* `output` is the feature map\\n* `W` is the filter (kernel)\\n* `input` is the input image\\n* `b` is the bias term\\n* `σ` is the activation function (e.g., ReLU)\\n\\nThe convolutional layer has several key properties:\\n\\n* **Spatial Hierarchies**: The filters are applied at multiple scales, allowing the model to capture features at different levels of abstraction.\\n* **Shared Weights**: The same filter is applied to multiple locations in the image, reducing the number of parameters and improving generalization.\\n* **Local Connectivity**: Each output pixel is computed based on a small region of the input image, preserving spatial relationships.\\n\\n**Backpropagation and Optimization**\\n\\nDuring training, the CNN model is optimized using backpropagation, which computes the gradient of the loss function with respect to the model's parameters. The optimization algorithm (e.g., Stochastic Gradient Descent, Adam) updates the model's parameters to minimize the loss function.\\n\\nThe backpropagation process involves:\\n\\n1. **Forward Pass**: The input image is passed through the network, computing the output and loss.\\n2. **Backward Pass**: The error is propagated backwards, computing the gradients of the loss with respect to each layer's parameters.\\n3. **Weight Update**: The model's parameters are updated based on the gradients and the optimization algorithm.\\n\\n**Convolutional Neural Network Example**\\n\\nLet's consider a simple example of a CNN architecture for image classification:\\n\\n* Input: 28x28 grayscale images\\n* Convolutional Layer 1: 32 filters, 3x3 kernel, stride 1\\n* Activation Function: ReLU\\n* Pooling Layer 1: 2x2 max pooling, stride 2\\n* Convolutional Layer 2: 64 filters, 3x3 kernel, stride 1\\n* Activation Function: ReLU\\n* Pooling Layer 2: 2x2 max pooling, stride 2\\n* Flattening Layer: 128 units\\n* Fully Connected Layer: 10 units (output layer)\\n\\nThis example illustrates the basic components of a CNN architecture, including convolutional layers, activation functions, pooling layers, flattening layers, and fully connected layers.\\n\\n**Conclusion**\\n\\nConvolutional Neural Networks (CNNs) are a powerful tool for image classification tasks. By leveraging spatial hierarchies, shared weights, and local connectivity, CNNs can learn complex features and patterns in images. The technical details of CNNs, including convolutional layers, activation functions, pooling layers, and backpropagation, are crucial to understanding how these models work. By grasping these concepts, developers and researchers can design and implement effective CNN architectures for a wide range of computer vision applications.\", 'role': 'user', 'name': 'Assistant2'}], summary=\"Let's dive into a detailed technical explanation of a complex topic: **Convolutional Neural Networks (CNNs) for Image Classification**.\\n\\n**Introduction**\\n\\nConvolutional Neural Networks (CNNs) are a type of deep learning model that have revolutionized the field of computer vision. They are particularly well-suited for image classification tasks, where the goal is to assign a label or category to an input image. In this explanation, we'll delve into the technical details of CNNs and explore how they work.\\n\\n**Architecture Overview**\\n\\nA typical CNN architecture consists of several layers:\\n\\n1. **Convolutional Layers**: These layers apply a set of learnable filters to the input image, scanning the image in a sliding window fashion. Each filter computes a feature map, which represents the presence of a particular feature at different locations in the image.\\n2. **Activation Functions**: The output of the convolutional layer is passed through an activation function, such as ReLU (Rectified Linear Unit) or Sigmoid, to introduce non-linearity into the model.\\n3. **Pooling Layers**: These layers downsample the feature maps, reducing the spatial dimensions while retaining the most important information.\\n4. **Flattening Layers**: The output of the convolutional and pooling layers is flattened into a 1D array, preparing the data for the fully connected layers.\\n5. **Fully Connected Layers**: These layers are used for classification, where the flattened output is fed into a neural network to produce a probability distribution over the classes.\\n\\n**Convolutional Layers**\\n\\nConvolutional layers are the core component of CNNs. They consist of a set of filters, each with a fixed size (e.g., 3x3 or 5x5) and a stride (e.g., 1 or 2). The filters are applied to the input image, scanning it in a sliding window fashion, to compute the feature maps.\\n\\nThe convolutional layer can be mathematically represented as:\\n\\n`output = σ(W * input + b)`\\n\\nwhere:\\n\\n* `output` is the feature map\\n* `W` is the filter (kernel)\\n* `input` is the input image\\n* `b` is the bias term\\n* `σ` is the activation function (e.g., ReLU)\\n\\nThe convolutional layer has several key properties:\\n\\n* **Spatial Hierarchies**: The filters are applied at multiple scales, allowing the model to capture features at different levels of abstraction.\\n* **Shared Weights**: The same filter is applied to multiple locations in the image, reducing the number of parameters and improving generalization.\\n* **Local Connectivity**: Each output pixel is computed based on a small region of the input image, preserving spatial relationships.\\n\\n**Backpropagation and Optimization**\\n\\nDuring training, the CNN model is optimized using backpropagation, which computes the gradient of the loss function with respect to the model's parameters. The optimization algorithm (e.g., Stochastic Gradient Descent, Adam) updates the model's parameters to minimize the loss function.\\n\\nThe backpropagation process involves:\\n\\n1. **Forward Pass**: The input image is passed through the network, computing the output and loss.\\n2. **Backward Pass**: The error is propagated backwards, computing the gradients of the loss with respect to each layer's parameters.\\n3. **Weight Update**: The model's parameters are updated based on the gradients and the optimization algorithm.\\n\\n**Convolutional Neural Network Example**\\n\\nLet's consider a simple example of a CNN architecture for image classification:\\n\\n* Input: 28x28 grayscale images\\n* Convolutional Layer 1: 32 filters, 3x3 kernel, stride 1\\n* Activation Function: ReLU\\n* Pooling Layer 1: 2x2 max pooling, stride 2\\n* Convolutional Layer 2: 64 filters, 3x3 kernel, stride 1\\n* Activation Function: ReLU\\n* Pooling Layer 2: 2x2 max pooling, stride 2\\n* Flattening Layer: 128 units\\n* Fully Connected Layer: 10 units (output layer)\\n\\nThis example illustrates the basic components of a CNN architecture, including convolutional layers, activation functions, pooling layers, flattening layers, and fully connected layers.\\n\\n**Conclusion**\\n\\nConvolutional Neural Networks (CNNs) are a powerful tool for image classification tasks. By leveraging spatial hierarchies, shared weights, and local connectivity, CNNs can learn complex features and patterns in images. The technical details of CNNs, including convolutional layers, activation functions, pooling layers, and backpropagation, are crucial to understanding how these models work. By grasping these concepts, developers and researchers can design and implement effective CNN architectures for a wide range of computer vision applications.\", cost={'usage_including_cached_inference': {'total_cost': 0.0, 'llama-3.3-70b-versatile': {'cost': 0.0, 'prompt_tokens': 47, 'completion_tokens': 972, 'total_tokens': 1019}}, 'usage_excluding_cached_inference': {'total_cost': 0.0, 'llama-3.3-70b-versatile': {'cost': 0.0, 'prompt_tokens': 47, 'completion_tokens': 972, 'total_tokens': 1019}}}, human_input=['exit'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen import UserProxyAgent,AssistantAgent\n",
    "\n",
    "\n",
    "configure_list = [ {\n",
    "    \"model\" : \"llama-3.3-70b-versatile\",\n",
    "    \"api_key\" : os.getenv(\"GROQ_API_KEY\"),\n",
    "    \"api_type\" : \"groq\"\n",
    "}]\n",
    "\n",
    "\n",
    "assitant1 = AssistantAgent(\n",
    "    name = \"assitant1\",\n",
    "    system_message = \"You provide concepts in simple terms\",\n",
    "    llm_config = {\"config_list\":configure_list}\n",
    ")\n",
    "\n",
    "assistant2 = AssistantAgent(\n",
    "    name = \"Assistant2\",\n",
    "    system_message = \"You Provide detailed technical explanations\",\n",
    "    llm_config = {\"config_list\":configure_list}\n",
    ")\n",
    "\n",
    "user = UserProxyAgent(name = \"user\", code_execution_config=False)\n",
    "\n",
    "user.initiate_chat(\n",
    "    assitant1,\n",
    "    message = \"Explain machine learning to me in simple terms.\",\n",
    "    auto_reply = True\n",
    ")\n",
    "\n",
    "user.initiate_chat(\n",
    "    assistant2,\n",
    "    message = \"Now give me a detailed technical explanation\",\n",
    "    auto_reply = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bb0a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from autogen import AssistantAgent,UserProxyAgent\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f913cf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder Created at : data_analysis\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "work_dir = Path(\"data_analysis\")\n",
    "work_dir.mkdir(exist_ok = True)\n",
    "\n",
    "print(f\"Folder Created at : {work_dir}\")\n",
    "\n",
    "df = pd.read_csv(\"../data/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c878e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = AssistantAgent(\n",
    "    name = \"assistant\",\n",
    "    system_message = \"You are a helpful AI Assistant that generates Python code for data analsysis\",\n",
    "    llm_config={\"config_list\":configure_list}\n",
    ")\n",
    "\n",
    "code_executor = LocalCommandLineCodeExecutor(work_dir=work_dir)\n",
    "user_proxy = UserProxyAgent(\n",
    "    name = \"user_proxy\",\n",
    "    code_execution_config={\"executor\":code_executor}\n",
    ")\n",
    "\n",
    "response = user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message = (\n",
    "        \"Generate Python code that : \\n\"\n",
    "        \"1. Loads the csv data.csv in\\n\"\n",
    "        \"2. cleans missing values by filling numeric \"\n",
    "        \"3. Prints a summary of the dataset includinf head , describe() , and column \"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
